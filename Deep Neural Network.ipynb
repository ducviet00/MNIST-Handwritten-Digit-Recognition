{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./digit-recognizer/train.csv\")\n",
    "#test = pd.read_csv(\"/content/drive/My Drive/20192/digit-recognizer/test.csv\")\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1)\n",
    "\n",
    "# Free some space\n",
    "del train\n",
    "\n",
    "# Normalize the data\n",
    "X_train = np.array(X_train) / 255.0\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes, dtype='float32'):\n",
    "    #from keras utils\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot coding\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "Y_val = to_categorical(Y_val, num_classes = 10)\n",
    "\n",
    "# Transpose X, Y\n",
    "X_train = np.array(X_train.T)\n",
    "Y_train = np.array(Y_train.T)\n",
    "X_val = np.array(X_val.T)\n",
    "Y_val = np.array(Y_val.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (784, 37800)\n",
      "Y_train.shape = (10, 37800)\n",
      "X_val.shape = (784, 4200)\n",
      "Y_val.shape = (10, 4200)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of X, Y\n",
    "print (\"X_train.shape = \" + str(X_train.shape)\n",
    "        + \"\\nY_train.shape = \" + str(Y_train.shape))\n",
    "print (\"X_val.shape = \" + str(X_val.shape)\n",
    "        + \"\\nY_val.shape = \" + str(Y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range (1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros(shape=(layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    print(\"---\")\n",
    "    print(W.shape)\n",
    "    print(A.shape)\n",
    "    print(b.shape)\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    \n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta(Z):\n",
    "    T = np.exp(Z) / np.sum(np.exp(Z), axis = 0)\n",
    "    return T, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    R = np.maximum(0, Z)\n",
    "    return R, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"theta\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = theta(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, Z)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A, W=parameters['W' + str(l)], b=parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache) \n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, W=parameters['W' + str(l+1)], b=parameters['b' + str(l+1)], activation=\"theta\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache):\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, Y, cache, activation):\n",
    "    linear_cache, Z = cache \n",
    "    A, W, b = linear_cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0 ] = 0\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    current_cache = caches[L-1]\n",
    "    linear_cache, Z = current_cache\n",
    "    dZ = AL - Y\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db\n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA=grads[\"dA\" + str(l+1)], Y=Y, cache=current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads['db' + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((10,m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) \n",
    "\n",
    "    #Partition \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size:\n",
    "                                  (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k+1)\n",
    "                                    * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size !=0:\n",
    "        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches\n",
    "                                    * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches\n",
    "                                    * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, num_epochs = 3000, learning_rate = 0.05, mini_batch_size = 64, print_cost = False):\n",
    "  parameters = initialize_parameters_deep(layers_dims)\n",
    "  seed = 10\n",
    "  costs = []\n",
    "\n",
    "  for i in range(num_epochs):\n",
    "    seed = seed + 1\n",
    "    minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "    cost_total = 0\n",
    "\n",
    "    for minibatch in minibatches:\n",
    "      #Select a minibatch\n",
    "      (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "      #foward propagation\n",
    "      AL, caches = L_model_forward(minibatch_X, parameters)\n",
    "\n",
    "      cost_total += compute_cost(AL, minibatch_Y)\n",
    "\n",
    "      grads = L_model_backward(AL, minibatch_Y, caches)\n",
    "      #update parameter\n",
    "      parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "    cost_avg = cost_total / len(minibatches)\n",
    "    if print_cost:\n",
    "      print (\"Cost after iteration %i: %f\" % (i, cost_avg))\n",
    "    if print_cost:\n",
    "      costs.append(cost_avg)\n",
    "\n",
    "  plt.plot(costs)\n",
    "  plt.ylabel('cost')\n",
    "  plt.xlabel('epochs (per 100)')\n",
    "  plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "  plt.show()\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [784, 64, 30, 18, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "(64, 784)\n",
      "(784, 32)\n",
      "(64, 1)\n",
      "---\n",
      "(30, 64)\n",
      "(64, 32)\n",
      "(30, 1)\n",
      "---\n",
      "(18, 30)\n",
      "(30, 32)\n",
      "(18, 1)\n",
      "---\n",
      "(10, 18)\n",
      "(18, 32)\n",
      "(10, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 18 but corresponding boolean dimension is 30",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-977dad7959b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m dnn_parameters = L_layer_model(X_train, Y_train, layers_dims, num_epochs = 1,\n\u001b[1;32m----> 2\u001b[1;33m                                learning_rate=0.05, mini_batch_size=32, print_cost = True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-da4fb8660d70>\u001b[0m in \u001b[0;36mL_layer_model\u001b[1;34m(X, Y, layers_dims, num_epochs, learning_rate, mini_batch_size, print_cost)\u001b[0m\n\u001b[0;32m     18\u001b[0m       \u001b[0mcost_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m       \u001b[1;31m#update parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-51092d10918a>\u001b[0m in \u001b[0;36mL_model_backward\u001b[1;34m(AL, Y, caches)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdA_prev_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurrent_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-1384b46b6ce3>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[1;34m(dA, Y, cache, activation)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mZ\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"theta\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 18 but corresponding boolean dimension is 30"
     ]
    }
   ],
   "source": [
    "dnn_parameters = L_layer_model(X_train, Y_train, layers_dims, num_epochs = 1,\n",
    "                               learning_rate=0.05, mini_batch_size=32, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
